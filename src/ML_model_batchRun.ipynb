{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8adcaf7",
   "metadata": {},
   "source": [
    "## GRAIN Dataset - ML Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf610a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import joblib\n",
    "import json\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from pyrosm import OSM\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "from shapely.strtree import STRtree\n",
    "from rasterio.sample import sample_gen\n",
    "from core.get_sword_reach_id import get_sword_reach\n",
    "from core.elevation_and_slope import compute_elevDiff_and_slope\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopandas.tools import sjoin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Specifying all data file paths and formats\n",
    "osm_data_folder = '../assets/osm_waterways/geoParquet/latest_record/'\n",
    "sword_data_folder = '../assets/SWORD_v16_shp/shp'\n",
    "sword_fileName_format = '{}_sword_reaches_hb{}_v16.shp'\n",
    "hydrobasins_l2_folder = '../assets/HydroBasins_world_L2'\n",
    "hydrobasin_l6_file = '../assets/supporting_data/hydrobasins_allBasins_l6_geoParquet_EPSG4326.parquet'\n",
    "hydrobasins_l2_fileName_format = 'hybas_{}_lev02_v1c.shp'\n",
    "world_countries_filePath = '../assets/supporting_data/world-administrative-boundaries.geojson'\n",
    "waterways_save_folder = '../assets/osm_waterways'\n",
    "sword_continent_map = '../assets/supporting_data/sword_continents.json'\n",
    "koppen_class_map = '../assets/supporting_data/koppen_class_label.json'\n",
    "koppen_geiger_fp = '../assets/koppen_geiger_data/koppen_geiger_0p00833333.cog'\n",
    "dem_cog_fp = '../assets/dem_data/World_e-Atlas-UCSD_SRTM30-plus_v8.cog'\n",
    "\n",
    "ml_training_data_saveFolder = '../assets/outputs/ML_training_data'\n",
    "esa_cci_cog_path = \"../assets/ESACCI/ESACCI-LC-L4-LCCS-Map-300m-P1Y-2015-v2.0.7.cog\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20131cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to safely sample data from a dataframe by iteration\n",
    "def safe_sample(df, columns, n, label):\n",
    "    while n > 0:\n",
    "        try:\n",
    "            return df[columns].sample(n=n, random_state=22)\n",
    "        except ValueError:\n",
    "            print(f\"Not enough data in {label} for {n} samples, trying {n-50}\")\n",
    "            n -= 50\n",
    "    print(f\" Failed to sample any data from {label}\")\n",
    "    return df[columns].head(1).iloc[0:0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9627ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to compute the features for ML Training and Application\n",
    "\n",
    "def vertex_per_length(geom):\n",
    "    try:\n",
    "        if geom.geom_type == 'LineString':\n",
    "            num_vertices = len(geom.coords)\n",
    "            length = geom.length\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            num_vertices = sum(len(part.coords) for part in geom.geoms)\n",
    "            length = geom.length\n",
    "        else:\n",
    "            return np.nan  # for unsupported types\n",
    "\n",
    "        return num_vertices / (length / 100) if length > 0 else np.nan\n",
    "\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_straightness_ratio(geom):\n",
    "    try:\n",
    "        # Handle MultiLineString by combining all coordinates\n",
    "        if geom.geom_type == 'MultiLineString':\n",
    "            # print(\"MultiLineString\")\n",
    "            coords = [pt for line in geom.geoms for pt in line.coords]\n",
    "            # print(coords)\n",
    "        else:\n",
    "            coords = list(geom.coords)\n",
    "\n",
    "        if len(coords) > 3:\n",
    "            start = coords[0]\n",
    "            end = coords[-1]\n",
    "            mid = coords[len(coords) // 2]\n",
    "\n",
    "            # print(start, mid, end)\n",
    "\n",
    "            dist1 = math.dist(start, mid)\n",
    "            dist2 = math.dist(mid, end)\n",
    "            simplified_length = dist1 + dist2\n",
    "        else:\n",
    "            start = coords[0]\n",
    "            end = coords[1]\n",
    "            simplified_length = math.dist(start, end)\n",
    "        \n",
    "        total_length = geom.length\n",
    "        # print(total_length)\n",
    "        sinousity = simplified_length / total_length # if total_length > 0 else np.nan\n",
    "        \n",
    "        # print(\"Simplified length:\", simplified_length, \"Total length:\", total_length, \"Sinousity:\", sinousity)\n",
    "        return sinousity\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def mean_turning_angle(geom):\n",
    "    def angle_at_b(a, b, c):\n",
    "        ba = (a[0] - b[0], a[1] - b[1])\n",
    "        bc = (c[0] - b[0], c[1] - b[1])\n",
    "        dot = ba[0]*bc[0] + ba[1]*bc[1]\n",
    "        det = ba[0]*bc[1] - ba[1]*bc[0]\n",
    "        angle = math.atan2(abs(det), dot)\n",
    "        return (180 - angle*180/math.pi)\n",
    "\n",
    "    try:\n",
    "        if geom.geom_type == \"MultiLineString\":\n",
    "            coords = [pt for line in geom.geoms for pt in line.coords]\n",
    "            coords = [coords[i] for i in range(len(coords)) if i == 0 or coords[i] != coords[i-1]]\n",
    "\n",
    "        else:\n",
    "            coords = list(geom.coords)\n",
    "\n",
    "        if len(coords) < 3:\n",
    "            return 0\n",
    "\n",
    "        angles = [\n",
    "            angle_at_b(coords[i - 1], coords[i], coords[i + 1])\n",
    "            for i in range(1, len(coords) - 1)\n",
    "        ]\n",
    "        mean_turn_angle = np.mean(angles) if len(angles) > 0 else 0\n",
    "        # print(mean_turn_angle)\n",
    "        return mean_turn_angle\n",
    "        \n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_curvature_index(geom):\n",
    "    \n",
    "\n",
    "    def angle_at_b(a, b, c):\n",
    "        # Convert to vectors\n",
    "        ba = (a[0] - b[0], a[1] - b[1])\n",
    "        bc = (c[0] - b[0], c[1] - b[1])\n",
    "        # Dot and cross products\n",
    "        dot = ba[0]*bc[0] + ba[1]*bc[1]\n",
    "        det = ba[0]*bc[1] - ba[1]*bc[0]\n",
    "\n",
    "        # Angle in radians\n",
    "        angle = math.atan2(abs(det), dot)\n",
    "        # print(angle)\n",
    "        return(180 - angle*180/math.pi)\n",
    "        # return angle  # Always positive\n",
    "\n",
    "    try:\n",
    "        # Flatten coords\n",
    "        if geom.geom_type == \"MultiLineString\":\n",
    "            coords = [pt for line in geom.geoms for pt in line.coords]\n",
    "            #removing duplicates\n",
    "            coords = [coords[i] for i in range(len(coords)) if i == 0 or coords[i] != coords[i-1]]\n",
    "            # print(coords)\n",
    "            # print(len(coords))\n",
    "            # print(type(coords))\n",
    "        else:\n",
    "            # print(\"LineString\")\n",
    "            coords = list(geom.coords)\n",
    "            # print(coords)\n",
    "            # print(type(coords))\n",
    "        \n",
    "\n",
    "        if len(coords) < 3:\n",
    "            return 0\n",
    "\n",
    "        angles = [\n",
    "            angle_at_b(coords[i - 1], coords[i], coords[i + 1])\n",
    "            for i in range(1, len(coords) - 1)\n",
    "        ]\n",
    "        # print(angles)\n",
    "        total_angle_change = sum(angles)\n",
    "        if geom.geom_type == 'LineString':\n",
    "            num_vertices = len(geom.coords)\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            # num_vertices = sum(len(part.coords) for part in geom.geoms)\n",
    "            num_vertices = len(coords)\n",
    "            \n",
    "        line_length_100m = geom.length / 100  # assuming EPSG:3857\n",
    "        # print(line_length_100m)\n",
    "        # return total_angle_change / (num_vertices-1)\n",
    "        return total_angle_change / line_length_100m if line_length_100m > 0 else 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(e))\n",
    "        traceback.print_exc()\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to identify koppen-geiger climate zone for a given canal vector\n",
    "def get_koppen_climate_class(canal_dataset):\n",
    "    koppen_climate_map = json.load(open(koppen_class_map))\n",
    "    koppen_data = rasterio.open(koppen_geiger_fp)\n",
    "    \n",
    "    canal_dataset_withKoppen = canal_dataset.copy()\n",
    "    def startpoint(line):\n",
    "        \n",
    "        if isinstance(line, LineString):\n",
    "            coords = list(line.coords)\n",
    "        elif isinstance(line, MultiLineString):\n",
    "            # flatten into a single coordinate list\n",
    "            coords = [pt for seg in line.geoms for pt in seg.coords]\n",
    "        else:\n",
    "            raise ValueError(\"Geometry is neither LineString nor MultiLineString\")\n",
    "        return coords[0]\n",
    "    canal_dataset_withKoppen[\"koppen_class_code\"] = pd.NA\n",
    "    # canal_dataset_withKoppen[\"koppen_class_desc\"] = pd.NA\n",
    "    error_count = 0\n",
    "    for idx, row in tqdm(canal_dataset_withKoppen.iterrows(), total=canal_dataset_withKoppen.shape[0], desc=\"Assigning Koppen Climate Class\"):\n",
    "        try:\n",
    "            first_coord = startpoint(row.geometry)\n",
    "            # print(first_coord)\n",
    "            koppen_value = koppen_data.sample([(first_coord[0], first_coord[1])])\n",
    "            koppen_value = int(list(koppen_value)[0])\n",
    "            koppen_string = str(koppen_value)\n",
    "\n",
    "            koppen_desc = koppen_climate_map[koppen_string]\n",
    "            canal_dataset_withKoppen.at[idx, \"koppen_class_code\"] = koppen_desc\n",
    "            # canal_dataset_withKoppen.at[idx, \"koppen_class_desc\"] = koppen_desc\n",
    "        except:\n",
    "            error_count = error_count + 1\n",
    "            continue\n",
    "    return canal_dataset_withKoppen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to add a unique grain id to each canal vector\n",
    "\n",
    "def add_GRAIN_id(final_canal_gdf, country_iso):\n",
    "    basins = gpd.read_parquet(hydrobasin_l6_file)[[\"PFAF_ID\", \"geometry\"]]\n",
    "    basins = basins.to_crs(final_canal_gdf.crs)\n",
    "    print(country_iso)\n",
    "    canals = gpd.sjoin(\n",
    "        final_canal_gdf,\n",
    "        basins,\n",
    "        predicate=\"intersects\",       \n",
    "        how=\"left\"                    \n",
    "    ).rename(columns={\"PFAF_ID\": \"pfaf_id\"})\n",
    "\n",
    "   \n",
    "    canals = canals.dropna(subset=[\"pfaf_id\"])\n",
    "    canals[\"pfaf_id\"] = canals[\"pfaf_id\"].astype(int)\n",
    "    canals[\"id_counter\"] = canals.groupby(\"pfaf_id\").cumcount() + 1\n",
    "    \n",
    "    canals[\"id_counter_str\"] = canals[\"id_counter\"].apply(lambda n: f\"{n:05d}\")\n",
    "\n",
    "    canals[\"grain_id\"] = (\n",
    "        country_iso + \"_\" +\n",
    "        canals[\"pfaf_id\"].astype(str) + \"_\" +\n",
    "        canals[\"id_counter_str\"]\n",
    "    )\n",
    "\n",
    "    return canals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dc07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to extract OSM tags\n",
    "def get_osm_name(tags):\n",
    "    if tags is None:\n",
    "        return None\n",
    "    try:\n",
    "        tag_dict_str = tags\n",
    "        tag_dict = json.loads(tag_dict_str)\n",
    "        if \"name\" not in tag_dict.keys():\n",
    "            return None\n",
    "        else:\n",
    "            return tag_dict[\"name\"]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def get_osm_source(tags):\n",
    "    if tags is None:\n",
    "        return None\n",
    "    try:\n",
    "        tag_dict_str = tags\n",
    "        tag_dict = json.loads(tag_dict_str)\n",
    "        if \"source:name\" not in tag_dict.keys():\n",
    "            return None\n",
    "        else:\n",
    "            return tag_dict[\"source:name\"]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_osm_name_fromOtherTags(other_tags):\n",
    "    if other_tags is None:\n",
    "        return None\n",
    "    try:\n",
    "        tag_raw = other_tags.split(\",\")\n",
    "        tag_dict_str = {}\n",
    "        for item in tag_raw:\n",
    "            key, value = item.strip('\"').split('\"=>\"', 1)\n",
    "            tag_dict_str[key] = value\n",
    "        # tag_dict = json.loads(tag_dict_str)\n",
    "        if \"name:en\" not in tag_dict_str.keys():\n",
    "            return None\n",
    "        else:\n",
    "            return tag_dict_str[\"name:en\"]\n",
    "    except:        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fucntion to perform topology based promotion of canal segments\n",
    "def promote_connected_canals_until_convergence(df, buffer_dist=10):\n",
    "    \"\"\"\n",
    "    Repeatedly promotes 'Canal_natural' segments to 'Canal_man_made (connected_round_X)'\n",
    "    if they intersect any currently man-made canal segment.\n",
    "\n",
    "    Parameters:\n",
    "        df : GeoDataFrame with a 'geometry' column and 'predicted_class' field\n",
    "        buffer_dist : distance in meters to consider for intersection (default 10)\n",
    "\n",
    "    Returns:\n",
    "        Updated GeoDataFrame with topologically promoted segments\n",
    "    \"\"\"\n",
    "    round_num = 1\n",
    "    total_promoted = -1  # force first run\n",
    "\n",
    "    while True:\n",
    "        print(f\"▶ Topology Promotion Round {round_num}\")\n",
    "\n",
    "        # Get current man-made segments\n",
    "        current_man_made = df[df[\"predicted_class\"].str.startswith(\"Canal_man_made\")].copy()\n",
    "        current_man_made_geoms = list(current_man_made.geometry.values)\n",
    "        tree = STRtree(current_man_made_geoms)\n",
    "\n",
    "        # Get current natural segments to test\n",
    "        to_test = df[df[\"predicted_class\"] == \"Canal_natural\"].copy()\n",
    "        if to_test.empty:\n",
    "            print(\"✅ No more natural segments left to test. Done.\")\n",
    "            break\n",
    "\n",
    "        promoted_idxs = []\n",
    "\n",
    "        for idx, geom in to_test.geometry.items():\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "            try:\n",
    "                geom_to_check = geom.buffer(buffer_dist) if buffer_dist > 0 else geom\n",
    "                candidate_ids = tree.query(geom_to_check)\n",
    "                candidate_geoms = [current_man_made_geoms[i] for i in candidate_ids]\n",
    "\n",
    "                if any(cand.intersects(geom_to_check) for cand in candidate_geoms):\n",
    "                    promoted_idxs.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"[[{idx}]]: Skipping due to error: {e}\")\n",
    "\n",
    "        if not promoted_idxs:\n",
    "            print(\"✅ No new connections found — stopping.\")\n",
    "            break\n",
    "\n",
    "        if round_num==10:\n",
    "            print(\"🔁 Maximum rounds reached (10). Stopping promotion.\")\n",
    "            break\n",
    "\n",
    "        label = f\"Canal_man_made (connected_round_{round_num})\"\n",
    "        df.loc[promoted_idxs, \"predicted_class\"] = label\n",
    "        print(f\"🔁 Promoted {len(promoted_idxs)} segments to: {label}\")\n",
    "\n",
    "        round_num += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#snapping end point within threshold distance of the nearest canal\n",
    "from shapely.geometry import Point\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import MultiPoint\n",
    "from shapely.ops import snap\n",
    "\n",
    "\n",
    "def get_endpoints(geom):\n",
    "    if geom.geom_type == 'LineString':\n",
    "        return Point(geom.coords[0]), Point(geom.coords[-1])\n",
    "    elif geom.geom_type == 'MultiLineString':\n",
    "        parts = geom.geoms\n",
    "        return Point(parts[0].coords[0]), Point(parts[-1].coords[-1])\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_canal_use(grain_data_canals, essa_cci_cog_path):\n",
    "    # print('Entered func')\n",
    "    counter = 0\n",
    "    cropland_class = [10, 11, 12, 20, 30, 130]\n",
    "    urban_andBare_class = [190,200,201,202]\n",
    "    water_class = [210]\n",
    "    grain_data_canals= grain_data_canals.to_crs(epsg=4326)\n",
    "    grain_data_canals['canal_use'] = pd.NA\n",
    "    src = rasterio.open(essa_cci_cog_path) \n",
    "    for idx, row in tqdm(grain_data_canals.iterrows(), total=grain_data_canals.shape[0], desc=\"Processing canal segments\" ):\n",
    "\n",
    "        geom = row.geometry.buffer(.01) #5km buffer\n",
    "        data, _ = mask(src, [geom], crop=True)           # data.shape = (1, h, w)\n",
    "        values  = data[0].ravel()\n",
    "        values  = values[values != src.nodata]\n",
    "\n",
    "        class_counts = np.bincount(values)\n",
    "        idx_sorted = class_counts.argsort()[::-1]\n",
    "        majority_class = idx_sorted[0]\n",
    "        second_major_class = idx_sorted[1] if class_counts[idx_sorted[1]] > 0 else None\n",
    "        # print(row.id, majority_class)\n",
    "        if majority_class in cropland_class:\n",
    "            canal_use_case = \"Agricultural\"\n",
    "            # print(\"Agricultural\")\n",
    "        elif majority_class in urban_andBare_class:\n",
    "            if second_major_class in cropland_class:\n",
    "                canal_use_case = \"Agricultural\"\n",
    "            else:\n",
    "                if majority_class in [200,201,202]:\n",
    "                    canal_use_case = \"Other\"\n",
    "                else:\n",
    "                    canal_use_case = \"Urban Waterway\"\n",
    "            # print(\"Urban\")\n",
    "        elif majority_class in water_class:\n",
    "            if second_major_class in cropland_class:\n",
    "                canal_use_case = \"Agricultural\"\n",
    "            else:\n",
    "                canal_use_case = \"Navigational Waterway\"\n",
    "            # print(\"Navigational\")\n",
    "        else:\n",
    "            if second_major_class in cropland_class:\n",
    "                canal_use_case = \"Agricultural\"\n",
    "            else:\n",
    "                canal_use_case = \"Other\"\n",
    "            # print(\"Natural\")\n",
    "        # print(majority_class,second_major_class,canal_use_case)\n",
    "        # print(canal_use_case)\n",
    "        grain_data_canals.at[idx, \"canal_use\"] = canal_use_case\n",
    "        counter += 1\n",
    "        \n",
    "    print('✔ Completed processing {} canal segments'.format(counter))\n",
    "    return grain_data_canals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major function to run the GRAIN ML model\n",
    "def run_grain_ml_model(country):\n",
    "    feature_cols = ['elev_diff','straightness_ratio','curvature_index_per_100m','mean_turning_angle','slope']\n",
    "\n",
    "    country_osm_waterways_data_fp = f'{osm_data_folder}{country.lower()}_waterway.parquet'\n",
    "    country_osm_waterways_data = gpd.read_parquet(country_osm_waterways_data_fp)\n",
    "    country_osm_waterways_data = country_osm_waterways_data[country_osm_waterways_data['geometry'].geom_type.isin(['LineString', 'MultiLineString'])]\n",
    "\n",
    "    country_osm_waterways_data = country_osm_waterways_data.to_crs(epsg=3857)\n",
    "    if \"osm_id\" in country_osm_waterways_data.columns:\n",
    "        country_osm_waterways_data.rename(columns={\"osm_id\": \"id\"}, inplace=True)\n",
    "        \n",
    "    if \"waterway\"  in country_osm_waterways_data.columns:\n",
    "            country_osm_waterways_data = country_osm_waterways_data.rename(columns={\"waterway\": \"osm_label\"})\n",
    "\n",
    "    if \"tags\" in country_osm_waterways_data.columns:\n",
    "        columns_to_keep = [\"id\", \"geometry\", \"osm_label\", \"tags\"]\n",
    "    elif \"other_tags\" in country_osm_waterways_data.columns:\n",
    "        columns_to_keep = [\"id\", \"geometry\", \"osm_label\", \"other_tags\"]\n",
    "    else:\n",
    "        columns_to_keep = [\"id\", \"geometry\", \"osm_label\"]\n",
    "    if \"name\" in country_osm_waterways_data.columns:\n",
    "        columns_to_keep.append(\"name\")\n",
    "\n",
    "    country_osm_canals = country_osm_waterways_data[country_osm_waterways_data['osm_label'].isin(['canal', 'ditch', 'drain'])]\n",
    "    country_osm_rivers = country_osm_waterways_data[country_osm_waterways_data['osm_label'].isin(['river', 'stream'])]\n",
    "    country_osm_rivers = country_osm_rivers[columns_to_keep]\n",
    "    country_osm_canals = country_osm_canals[columns_to_keep]\n",
    "\n",
    "    #computing features\n",
    "    print(f'[{country}]: Computing features. This might take a while...')\n",
    "    print(f'[{country}]: Rivers')\n",
    "\n",
    "    country_osm_rivers[\"straightness_ratio\"] = country_osm_rivers.geometry.apply(get_straightness_ratio)\n",
    "    country_osm_rivers[\"mean_turning_angle\"] = country_osm_rivers.geometry.apply(mean_turning_angle)\n",
    "    country_osm_rivers[\"curvature_index_per_100m\"] = country_osm_rivers.geometry.apply(get_curvature_index)\n",
    "    #compute length of the geometry in km\n",
    "    country_osm_rivers['length'] = country_osm_rivers['geometry'].length / 1E3\n",
    "    country_osm_rivers = compute_elevDiff_and_slope(dem_cog_fp,country_osm_rivers)\n",
    "\n",
    "\n",
    "    # do the same for canal\n",
    "    print(f'[{country}]: Canals')\n",
    "    country_osm_canals[\"straightness_ratio\"] = country_osm_canals.geometry.apply(get_straightness_ratio)\n",
    "    country_osm_canals[\"mean_turning_angle\"] = country_osm_canals.geometry.apply(mean_turning_angle)\n",
    "    country_osm_canals[\"curvature_index_per_100m\"] = country_osm_canals.geometry.apply(get_curvature_index)\n",
    "    #compute length of the geometry in km\n",
    "    country_osm_canals['length'] = country_osm_canals['geometry'].length / 1E3\n",
    "    country_osm_canals = compute_elevDiff_and_slope(dem_cog_fp,country_osm_canals)\n",
    "\n",
    "    #drop nans\n",
    "    cols_checkforNans = ['elev_diff', 'slope', 'straightness_ratio', 'mean_turning_angle', 'curvature_index_per_100m']\n",
    "    country_osm_rivers_clean = country_osm_rivers.dropna(subset=cols_checkforNans).copy()\n",
    "    country_osm_canals_clean = country_osm_canals.dropna(subset=cols_checkforNans).copy()\n",
    "\n",
    "    #loading random forest model\n",
    "    model_fp = f'{ml_training_data_saveFolder}/ML_model_random_forest.pkl'\n",
    "    model = joblib.load(model_fp)\n",
    "\n",
    "\n",
    "    X_country_osm_rivers = country_osm_rivers_clean[feature_cols]\n",
    "\n",
    "    #predict using random forest model\n",
    "    country_osm_rivers_clean[\"predicted_label\"] = model.predict(X_country_osm_rivers)\n",
    "    country_osm_rivers_clean[\"predicted_class\"] = country_osm_rivers_clean[\"predicted_label\"].map({0: \"river\", 1: \"canal\"})\n",
    "    #getting confidence\n",
    "    proba_rivers = model.predict_proba(X_country_osm_rivers)   \n",
    "    country_osm_rivers_clean[[\"prob_river\", \"prob_canal\"]] = proba_rivers\n",
    "\n",
    "    X_country_osm_canals = country_osm_canals_clean[feature_cols]\n",
    "    #predict using random forest model\n",
    "    country_osm_canals_clean[\"predicted_label\"] = model.predict(X_country_osm_canals)\n",
    "    country_osm_canals_clean[\"predicted_class\"] = country_osm_canals_clean[\"predicted_label\"].map({0: \"river\", 1: \"canal\"})\n",
    "    proba_canals = model.predict_proba(X_country_osm_canals)   \n",
    "    country_osm_canals_clean[[\"prob_river\", \"prob_canal\"]] = proba_canals\n",
    "\n",
    "\n",
    "    ##adding name, osm source, and alt name fields\n",
    "    #rivers\n",
    "    if \"name\" in country_osm_rivers_clean.columns:\n",
    "        country_osm_rivers_clean = country_osm_rivers_clean.rename(columns={\"name\": \"osm_name\"})\n",
    "    if \"tags\" in country_osm_rivers_clean.columns:\n",
    "        if \"osm_name\" not in country_osm_rivers_clean.columns:\n",
    "            country_osm_rivers_clean['osm_name'] = country_osm_rivers_clean['tags'].apply(get_osm_name)\n",
    "        country_osm_rivers_clean['osm_source'] = country_osm_rivers_clean['tags'].apply(get_osm_source) \n",
    "\n",
    "    if \"other_tags\" in country_osm_rivers_clean.columns:\n",
    "        if \"osm_name\" not in country_osm_rivers_clean.columns:\n",
    "            country_osm_rivers_clean['osm_name'] = country_osm_rivers_clean['other_tags'].apply(get_osm_name_fromOtherTags)\n",
    "        else:\n",
    "            country_osm_rivers_clean['alt_name'] = country_osm_rivers_clean['other_tags'].apply(get_osm_name_fromOtherTags)\n",
    "\n",
    "    if \"osm_source\" not in country_osm_rivers_clean.columns:\n",
    "        country_osm_rivers_clean['osm_source'] = None\n",
    "\n",
    "    if \"alt_name\" not in country_osm_rivers_clean.columns:\n",
    "        country_osm_rivers_clean['alt_name'] = None\n",
    "    #canals\n",
    "    if \"name\" in country_osm_canals_clean.columns:\n",
    "        country_osm_canals_clean = country_osm_canals_clean.rename(columns={\"name\": \"osm_name\"})\n",
    "    if \"tags\" in country_osm_canals_clean.columns:\n",
    "        if \"osm_name\" not in country_osm_canals_clean.columns:\n",
    "            country_osm_canals_clean['osm_name'] = country_osm_canals_clean['tags'].apply(get_osm_name)\n",
    "        country_osm_canals_clean['osm_source'] = country_osm_canals_clean['tags'].apply(get_osm_source) \n",
    "\n",
    "    if \"other_tags\" in country_osm_canals_clean.columns:\n",
    "        if \"osm_name\" not in country_osm_canals_clean.columns:\n",
    "            country_osm_canals_clean['osm_name'] = country_osm_canals_clean['other_tags'].apply(get_osm_name_fromOtherTags)\n",
    "        else:\n",
    "            country_osm_canals_clean['alt_name'] = country_osm_canals_clean['other_tags'].apply(get_osm_name_fromOtherTags)\n",
    "\n",
    "    if \"osm_source\" not in country_osm_canals_clean.columns:\n",
    "        country_osm_canals_clean['osm_source'] = None\n",
    "\n",
    "    if \"alt_name\" not in country_osm_canals_clean.columns:\n",
    "        country_osm_canals_clean['alt_name'] = None\n",
    "\n",
    "\n",
    "    #removing any sword rivers from canals\n",
    "\n",
    "    sword_continent_map_data = json.load(open(sword_continent_map))\n",
    "    random_canal = country_osm_canals_clean.sample(n=1, random_state=21)\n",
    "    country_iso, country_name_official, continent_name, sword_reach_id = get_sword_reach(random_canal)\n",
    "\n",
    "    sword_gdfs = []\n",
    "    print(f'[[{country}]]:Removing any sword rivers from canal dataset')\n",
    "    for sword_id in sword_reach_id:\n",
    "\n",
    "        for key, value in sword_continent_map_data.items():\n",
    "            if str(sword_id) in value:\n",
    "                continent_abbrv = key\n",
    "\n",
    "        sword_file_path = os.path.join(sword_data_folder,continent_abbrv,sword_fileName_format.format(continent_abbrv.lower(), sword_id))\n",
    "\n",
    "        sword_data_gpd = gpd.read_file(sword_file_path)\n",
    "        sword_gdfs.append(sword_data_gpd)\n",
    "\n",
    "    sword_data_gpd = pd.concat(sword_gdfs)\n",
    "    country_sword_mercator = sword_data_gpd.to_crs(epsg=3857)\n",
    "\n",
    "    country_osm_canals_clean = country_osm_canals_clean.to_crs(epsg=3857)\n",
    "    country_osm_canals_sword_intersection = gpd.sjoin(country_osm_canals_clean, country_sword_mercator, how=\"inner\", predicate=\"intersects\")\n",
    "    to_remove_index = country_osm_canals_sword_intersection.index\n",
    "    country_osm_canals_final = country_osm_canals_clean.drop(index=to_remove_index)\n",
    "\n",
    "    print(f'[[{country}]]:Running checks for any osm canals labeled as rivers')\n",
    "    country_osm_rivers_clean = country_osm_rivers_clean.to_crs(epsg=3857)\n",
    "    country_osm_rivers_toCheckForNamedCanals = country_osm_rivers_clean.copy()\n",
    "    cols = [\"osm_name\", \"alt_name\"]\n",
    "    CANAL_RE = r\"\\bcanal\\b\" \n",
    "\n",
    "    mask_namedCanals = (\n",
    "        country_osm_rivers_toCheckForNamedCanals[cols]\n",
    "        .apply(lambda s: s.str.contains(CANAL_RE, case=False, na=False))\n",
    "        .any(axis=1)                         \n",
    "    )\n",
    "    canals_to_merge = country_osm_rivers_toCheckForNamedCanals[mask_namedCanals]\n",
    "    canals_to_merge['predicted_class'] = 'canal'\n",
    "    print(\"Identified\", len(canals_to_merge), \"river segments that have tag 'canal' in its name. Merging with canals\")\n",
    "    country_osm_canals_final = pd.concat([country_osm_canals_clean, canals_to_merge], ignore_index=True)\n",
    "\n",
    "    print(f'[[{country}]]:Relabeling canals classified as rivers if they are connected to canals endpoints')\n",
    "    ml_canals_man_made = country_osm_canals_final[country_osm_canals_final[\"predicted_class\"] == \"canal\"].copy()\n",
    "    ml_canals_rivers = country_osm_canals_final[country_osm_canals_final[\"predicted_class\"] == \"river\"].copy()\n",
    "\n",
    "    # Collect all endpoints\n",
    "    man_made_endpoints = []\n",
    "    for geom in ml_canals_man_made.geometry:\n",
    "        start, end = get_endpoints(geom)\n",
    "        man_made_endpoints.extend([start, end])\n",
    "\n",
    "    # Create a spatial index for fast lookup\n",
    "    endpoint_tree = STRtree(man_made_endpoints)\n",
    "    man_made_geoms = list(ml_canals_man_made.geometry)\n",
    "    man_made_tree = STRtree(man_made_geoms)\n",
    "    #checking if the endpoints of canals classified as rivers are within a buffer distance of man-made canals. if so they are man-made canals\n",
    "    buffer_dist = 50  # in meters\n",
    "\n",
    "    promoted_idxs = []\n",
    "\n",
    "    for idx, geom in ml_canals_rivers.geometry.items():\n",
    "            if geom is None or geom.is_empty:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Buffer canal geom slightly to catch near-touches (optional)\n",
    "                geom_to_check = geom.buffer(buffer_dist) if buffer_dist > 0 else geom\n",
    "                \n",
    "                # Query STRtree for potential matches\n",
    "                candidates_id = man_made_tree.query(geom_to_check)\n",
    "                # Check if any intersect\n",
    "                #if candidates in not empty break out of loop\n",
    "                if len(candidates_id) > 0:\n",
    "                    candidate_geoms = [man_made_geoms[i] for i in candidates_id]\n",
    "                    if any(candidate.intersects(geom_to_check) for candidate in candidate_geoms):\n",
    "                        promoted_idxs.append(idx)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[[{idx}]]: Skipping due to error: {e}\")\n",
    "                traceback.print_exc()\n",
    "    print(f'Identified {len(promoted_idxs)} segments. Promoting...')\n",
    "\n",
    "    ml_canals_rivers.loc[promoted_idxs, \"predicted_class\"] = \"Canal_man_made (connected)\"\n",
    "    ml_canals_rivers.loc[~ml_canals_rivers.index.isin(promoted_idxs), \"predicted_class\"] = \"Canal_natural\"\n",
    "\n",
    "    #combining back\n",
    "    ml_canals_cleaned = pd.concat([ml_canals_man_made, ml_canals_rivers], ignore_index=True)\n",
    "\n",
    "    print(f'[[{country}]]:Using successive topology checks to promote canal regments labelled as canal_natural')\n",
    "    ml_canals_cleaned = promote_connected_canals_until_convergence(ml_canals_cleaned, buffer_dist=50)\n",
    "\n",
    "    print(\"Adding osm rivers classified as canals to river dataset based on intersection criteria\")\n",
    "    ml_canals_cleaned_preRiver = ml_canals_cleaned.copy()\n",
    "    ml_canals_cleaned_preRiver.loc[~ml_canals_cleaned_preRiver[\"predicted_class\"].isin([\"canal\", \"Canal_natural\"]), \"predicted_class\"] = \"canal\"\n",
    "\n",
    "    country_rivers_classified_man_made_canals = country_osm_rivers_clean[country_osm_rivers_clean[\"predicted_class\"] == \"canal\"]\n",
    "    river_man_made = country_rivers_classified_man_made_canals.copy()\n",
    "\n",
    "    canal_geoms = list(ml_canals_cleaned_preRiver.geometry.values)\n",
    "    canal_tree = STRtree(canal_geoms)\n",
    "\n",
    "    intersecting_river_idxs = []\n",
    "\n",
    "    joined = sjoin(river_man_made, ml_canals_cleaned_preRiver, how=\"left\", predicate=\"intersects\")\n",
    "    intersecting_idxs = joined[~joined.index_right.isna()].index\n",
    "    river_man_made.loc[intersecting_idxs, \"predicted_class\"] = \"Canal_man_made_connected\"\n",
    "\n",
    "    final_canal_dataset = pd.concat(\n",
    "            [ml_canals_cleaned_preRiver, river_man_made[river_man_made[\"predicted_class\"] == \"Canal_man_made_connected\"]],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "    final_canal_dataset.loc[final_canal_dataset['predicted_class']=='Canal_man_made_connected', 'predicted_class'] = 'canal'\n",
    "\n",
    "    print(f\"[[{country}]]:Running checks for any osm canals. Removes those that have 'River' in their OSM names\")\n",
    "    final_canal_dataset = final_canal_dataset.to_crs(epsg=3857)\n",
    "    final_canal_dataset_toCheckForNamedRivers = final_canal_dataset.copy()\n",
    "    cols = [\"osm_name\", \"alt_name\"]\n",
    "    RIVER_RE = r\"\\briver\\b\" \n",
    "    CANAL_RE = r\"\\bcanal\\b\"\n",
    "    mask_namedRivers = (\n",
    "        final_canal_dataset_toCheckForNamedRivers[cols]\n",
    "        .apply(lambda s: s.str.contains(RIVER_RE, case=False, na=False))\n",
    "        .any(axis=1)                         \n",
    "    )\n",
    "    mask_namedCanals = (\n",
    "        final_canal_dataset_toCheckForNamedRivers[cols]\n",
    "        .apply(lambda s: s.str.contains(CANAL_RE, case=False, na=False))\n",
    "        .any(axis=1)\n",
    "    )\n",
    "    mask_to_drop = mask_namedRivers & ~mask_namedCanals\n",
    "\n",
    "    final_canal_dataset_clean = final_canal_dataset_toCheckForNamedRivers.loc[~mask_to_drop].copy()\n",
    "    #Assigning Canal use case based on ES CCI LULC Dataset 2015\n",
    "\n",
    "    print(f'[[{country}]]: Assigning canal use case based on ESA CCI LULC data')\n",
    "    final_canal_dataset_withUseCase = assign_canal_use(final_canal_dataset_clean, esa_cci_cog_path)\n",
    "    print(f'[[{country}]]: Adding GRAIN ID in the format ISO3-code_PFAF Level 6 ID_sequential numbering')\n",
    "    final_canal_dataset_withGrainID = add_GRAIN_id(final_canal_dataset_withUseCase, country_iso[0])\n",
    "    print(f'[[{country}]]: Adding country and continent names and Koppen Climate Class')\n",
    "    final_canal_dataset_withGrainID['country'] = country_name_official[0]\n",
    "    final_canal_dataset_withGrainID['continent'] = continent_name[0]\n",
    "    final_canal_dataset_withGrainID['country_iso'] = country_iso[0]\n",
    "    final_canal_dataset_withGrainID['update_date'] = \"2025-07-31\"\n",
    "    final_canal_dataset_withGrainID['version'] = \"v.1.0.0\"\n",
    "    final_canal_dataset_withKoppenClass = get_koppen_climate_class(final_canal_dataset_withGrainID)\n",
    "    print(f'[[{country}]]: Trimming and renaming columns')\n",
    "    columns_to_keep = [\"grain_id\", \"id\", \"country\", \"continent\", \"country_iso\", \"length\",\"elev_diff\",\"slope\",\"predicted_class\",\"prob_canal\",\n",
    "    \"osm_name\",\"osm_label\", \"tags\",\"osm_source\", \"alt_name\", \"canal_use\",\"koppen_class_code\", \"update_date\",\"version\", \"geometry\"]\n",
    "    if \"tags\" not in final_canal_dataset_withKoppenClass.columns:\n",
    "        final_canal_dataset_withKoppenClass[\"tags\"] = None\n",
    "\n",
    "    final_grain_canal_dataset = final_canal_dataset_withKoppenClass[columns_to_keep]\n",
    "    final_grain_canal_dataset = final_grain_canal_dataset.rename(columns={\"id\": \"osm_id\", \"slope\": \"slope_MKM\",\n",
    "    \"length\": \"length_KM\", \"elev_diff\": \"elev_diff_M\", \"prob_canal\": \"confidence\", })\n",
    "\n",
    "    print(f'[[{country}]]:Saving final dataset for {country} to parquet and geojson file')\n",
    "    final_grain_canal_dataset.to_parquet(f\"../assets/outputs/final_outputs_withUseCase/{country}_GRAIN_v.1.0.parquet\")\n",
    "    final_grain_canal_dataset.to_file(f\"../assets/outputs/final_outputs_withUseCase/{country}_GRAIN_v.1.0.geojson\", driver='GeoJSON')\n",
    "    print(f'[[{country}]]:Completed')\n",
    "    print('==========================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0094fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_files = os.listdir(osm_data_folder)\n",
    "error_list = []\n",
    "output_folder = '../assets/outputs/final_outputs_withUseCase/'\n",
    "counter_country = 1\n",
    "for file in osm_files[0:]:\n",
    "    if file.endswith('_waterway.parquet'):\n",
    "            country_name = file.split('_waterway.parquet')[0]\n",
    "            print(f\"Processing country: {counter_country}/115: {country_name}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping file: {file}\")\n",
    "        error_list.append(file)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        save_path = os.path.join(output_folder, f\"{country_name}_GRAIN_v.1.0.parquet\")\n",
    "        \n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"[[{country_name}]]: File already exists: {save_path}, skipping...\")\n",
    "            print('==========================================================================')\n",
    "            continue\n",
    "        run_grain_ml_model(country_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {country_name}: {e}\")\n",
    "        counter_country += 1\n",
    "        error_list.append(country_name)\n",
    "        traceback.print_exc()\n",
    "    else:\n",
    "        counter_country += 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".grain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
